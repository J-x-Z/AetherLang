// Automatic Differentiation Library for AetherLang
// Tape-based reverse-mode autodiff for neural network training
//
// Design: Each operation is recorded on a tape, then backward() traverses
// the tape in reverse order to compute gradients via chain rule.

extern "C" {
    fn malloc(size: u64) -> *u8;
    fn free(ptr: *u8);
    fn memset(dst: *u8, c: i32, n: u64) -> *u8;
}

// ==================== Operation Types ====================

pub const OP_NONE: i32 = 0      // Leaf node (input/parameter)
pub const OP_ADD: i32 = 1       // z = x + y
pub const OP_MUL: i32 = 2       // z = x * y
pub const OP_SUB: i32 = 3       // z = x - y
pub const OP_DIV: i32 = 4       // z = x / y
pub const OP_NEG: i32 = 5       // z = -x
pub const OP_EXP: i32 = 6       // z = exp(x)
pub const OP_LOG: i32 = 7       // z = log(x)
pub const OP_POW: i32 = 8       // z = x^y
pub const OP_RELU: i32 = 9      // z = max(0, x)
pub const OP_SIGMOID: i32 = 10  // z = 1/(1+exp(-x))
pub const OP_TANH: i32 = 11     // z = tanh(x)
pub const OP_SUM: i32 = 12      // z = sum(x)
pub const OP_MATMUL: i32 = 13   // z = x @ y

// ==================== Value Node ====================

/// A value in the computation graph with gradient tracking
pub struct Value {
    pub data: f32,          // Forward value
    pub grad: f32,          // Gradient (accumulated during backward)
    pub op: i32,            // Operation that created this value
    pub parent1: *Value,    // First parent (null for leaves)
    pub parent2: *Value,    // Second parent (null for unary ops)
    pub requires_grad: bool, // Whether to track gradients
}

impl Value {
    /// Create a new leaf value (parameter or input)
    pub fn new(data: f32) -> Value {
        return Value {
            data: data,
            grad: 0.0,
            op: OP_NONE,
            parent1: 0 as *Value,
            parent2: 0 as *Value,
            requires_grad: true,
        };
    }

    /// Create a constant (no gradient tracking)
    pub fn constant(data: f32) -> Value {
        return Value {
            data: data,
            grad: 0.0,
            op: OP_NONE,
            parent1: 0 as *Value,
            parent2: 0 as *Value,
            requires_grad: false,
        };
    }

    /// Zero the gradient
    pub fn zero_grad(self: *Value) {
        (*self).grad = 0.0;
    }
}

// ==================== Forward Operations ====================

/// Addition: z = x + y
pub fn add(x: *Value, y: *Value) -> Value {
    return Value {
        data: (*x).data + (*y).data,
        grad: 0.0,
        op: OP_ADD,
        parent1: x,
        parent2: y,
        requires_grad: (*x).requires_grad || (*y).requires_grad,
    };
}

/// Multiplication: z = x * y
pub fn mul(x: *Value, y: *Value) -> Value {
    return Value {
        data: (*x).data * (*y).data,
        grad: 0.0,
        op: OP_MUL,
        parent1: x,
        parent2: y,
        requires_grad: (*x).requires_grad || (*y).requires_grad,
    };
}

/// Subtraction: z = x - y
pub fn sub(x: *Value, y: *Value) -> Value {
    return Value {
        data: (*x).data - (*y).data,
        grad: 0.0,
        op: OP_SUB,
        parent1: x,
        parent2: y,
        requires_grad: (*x).requires_grad || (*y).requires_grad,
    };
}

/// Negation: z = -x
pub fn neg(x: *Value) -> Value {
    return Value {
        data: 0.0 - (*x).data,
        grad: 0.0,
        op: OP_NEG,
        parent1: x,
        parent2: 0 as *Value,
        requires_grad: (*x).requires_grad,
    };
}

/// ReLU: z = max(0, x)
pub fn relu(x: *Value) -> Value {
    let data: f32 = if (*x).data > 0.0 { (*x).data } else { 0.0 };
    return Value {
        data: data,
        grad: 0.0,
        op: OP_RELU,
        parent1: x,
        parent2: 0 as *Value,
        requires_grad: (*x).requires_grad,
    };
}

/// Sigmoid: z = 1 / (1 + exp(-x))
pub fn sigmoid(x: *Value) -> Value {
    // Approximation: sigmoid(x) ≈ 0.5 + 0.5 * tanh(x/2)
    let half_x: f32 = (*x).data * 0.5;
    let abs_x: f32 = if half_x < 0.0 { 0.0 - half_x } else { half_x };
    let tanh_approx: f32 = half_x / (1.0 + abs_x + half_x * half_x / 3.0);
    let data: f32 = 0.5 + 0.5 * tanh_approx;
    return Value {
        data: data,
        grad: 0.0,
        op: OP_SIGMOID,
        parent1: x,
        parent2: 0 as *Value,
        requires_grad: (*x).requires_grad,
    };
}

/// Tanh: z = tanh(x)
pub fn tanh(x: *Value) -> Value {
    let v: f32 = (*x).data;
    let abs_v: f32 = if v < 0.0 { 0.0 - v } else { v };
    let data: f32 = v / (1.0 + abs_v + v * v / 3.0);
    return Value {
        data: data,
        grad: 0.0,
        op: OP_TANH,
        parent1: x,
        parent2: 0 as *Value,
        requires_grad: (*x).requires_grad,
    };
}

// ==================== Backward Pass ====================

/// Compute gradients for a single node (local backward)
pub fn backward_node(node: *Value) {
    let op: i32 = (*node).op;
    let grad: f32 = (*node).grad;
    let p1: *Value = (*node).parent1;
    let p2: *Value = (*node).parent2;

    if op == OP_NONE {
        // Leaf node - nothing to propagate
        return;
    }

    if op == OP_ADD {
        // z = x + y  =>  dL/dx = dL/dz, dL/dy = dL/dz
        if p1 != 0 as *Value && (*p1).requires_grad {
            (*p1).grad = (*p1).grad + grad;
        }
        if p2 != 0 as *Value && (*p2).requires_grad {
            (*p2).grad = (*p2).grad + grad;
        }
    }

    if op == OP_MUL {
        // z = x * y  =>  dL/dx = dL/dz * y, dL/dy = dL/dz * x
        if p1 != 0 as *Value && (*p1).requires_grad {
            (*p1).grad = (*p1).grad + grad * (*p2).data;
        }
        if p2 != 0 as *Value && (*p2).requires_grad {
            (*p2).grad = (*p2).grad + grad * (*p1).data;
        }
    }

    if op == OP_SUB {
        // z = x - y  =>  dL/dx = dL/dz, dL/dy = -dL/dz
        if p1 != 0 as *Value && (*p1).requires_grad {
            (*p1).grad = (*p1).grad + grad;
        }
        if p2 != 0 as *Value && (*p2).requires_grad {
            (*p2).grad = (*p2).grad - grad;
        }
    }

    if op == OP_NEG {
        // z = -x  =>  dL/dx = -dL/dz
        if p1 != 0 as *Value && (*p1).requires_grad {
            (*p1).grad = (*p1).grad - grad;
        }
    }

    if op == OP_RELU {
        // z = relu(x)  =>  dL/dx = dL/dz if x > 0 else 0
        if p1 != 0 as *Value && (*p1).requires_grad {
            if (*p1).data > 0.0 {
                (*p1).grad = (*p1).grad + grad;
            }
            // else: gradient is 0, nothing to add
        }
    }

    if op == OP_SIGMOID {
        // z = sigmoid(x)  =>  dL/dx = dL/dz * z * (1 - z)
        if p1 != 0 as *Value && (*p1).requires_grad {
            let z: f32 = (*node).data;
            (*p1).grad = (*p1).grad + grad * z * (1.0 - z);
        }
    }

    if op == OP_TANH {
        // z = tanh(x)  =>  dL/dx = dL/dz * (1 - z^2)
        if p1 != 0 as *Value && (*p1).requires_grad {
            let z: f32 = (*node).data;
            (*p1).grad = (*p1).grad + grad * (1.0 - z * z);
        }
    }
}

// ==================== Tape for Automatic Backward ====================

/// Maximum number of operations in a tape
pub const TAPE_MAX_SIZE: u64 = 1024

/// Computation tape for recording operations
pub struct Tape {
    pub nodes: *Value,      // Array of value nodes
    pub len: u64,           // Number of nodes
    pub cap: u64,           // Capacity
}

impl Tape {
    /// Create a new empty tape
    pub fn new() -> Tape {
        let size: u64 = TAPE_MAX_SIZE * 48;  // sizeof(Value) ≈ 48
        let nodes: *Value = malloc(size) as *Value;
        memset(nodes as *u8, 0, size);
        return Tape {
            nodes: nodes,
            len: 0,
            cap: TAPE_MAX_SIZE,
        };
    }

    /// Add a value to the tape, returns pointer to stored value
    pub fn push(self: *Tape, v: Value) -> *Value {
        if (*self).len >= (*self).cap {
            // Tape full - just return null (should handle error)
            return 0 as *Value;
        }
        let idx: u64 = (*self).len;
        let ptr: *Value = ((*self).nodes as u64 + idx * 48) as *Value;
        *ptr = v;
        (*self).len = (*self).len + 1;
        return ptr;
    }

    /// Create a leaf value and add to tape
    pub fn var(self: *Tape, data: f32) -> *Value {
        return (*self).push(Value::new(data));
    }

    /// Create a constant and add to tape
    pub fn const_val(self: *Tape, data: f32) -> *Value {
        return (*self).push(Value::constant(data));
    }

    /// Zero all gradients in the tape
    pub fn zero_grad(self: *Tape) {
        let mut i: u64 = 0;
        while i < (*self).len {
            let ptr: *Value = ((*self).nodes as u64 + i * 48) as *Value;
            (*ptr).grad = 0.0;
            i = i + 1;
        }
    }

    /// Run backward pass from a loss value
    /// The loss should be the last value added to tape
    pub fn backward(self: *Tape, loss: *Value) {
        // Set gradient of loss to 1.0
        (*loss).grad = 1.0;

        // Traverse tape in reverse order
        let mut i: u64 = (*self).len;
        while i > 0 {
            i = i - 1;
            let ptr: *Value = ((*self).nodes as u64 + i * 48) as *Value;
            backward_node(ptr);
        }
    }

    /// Free tape memory
    pub fn drop(self: *Tape) {
        free((*self).nodes as *u8);
    }
}

// ==================== Tape Operations ====================

/// Add with tape recording
pub fn tape_add(tape: *Tape, x: *Value, y: *Value) -> *Value {
    let result: Value = add(x, y);
    return (*tape).push(result);
}

/// Multiply with tape recording
pub fn tape_mul(tape: *Tape, x: *Value, y: *Value) -> *Value {
    let result: Value = mul(x, y);
    return (*tape).push(result);
}

/// Subtract with tape recording
pub fn tape_sub(tape: *Tape, x: *Value, y: *Value) -> *Value {
    let result: Value = sub(x, y);
    return (*tape).push(result);
}

/// ReLU with tape recording
pub fn tape_relu(tape: *Tape, x: *Value) -> *Value {
    let result: Value = relu(x);
    return (*tape).push(result);
}

/// Sigmoid with tape recording
pub fn tape_sigmoid(tape: *Tape, x: *Value) -> *Value {
    let result: Value = sigmoid(x);
    return (*tape).push(result);
}

/// Tanh with tape recording
pub fn tape_tanh(tape: *Tape, x: *Value) -> *Value {
    let result: Value = tanh(x);
    return (*tape).push(result);
}

// ==================== Loss Functions ====================

/// Mean Squared Error: (pred - target)^2
pub fn mse_loss(tape: *Tape, pred: *Value, target: *Value) -> *Value {
    let diff: *Value = tape_sub(tape, pred, target);
    return tape_mul(tape, diff, diff);
}

// ==================== Optimizer ====================

/// SGD update: param = param - lr * grad
pub fn sgd_step(params: *Value, num_params: u64, lr: f32) {
    let mut i: u64 = 0;
    while i < num_params {
        let ptr: *Value = (params as u64 + i * 48) as *Value;
        if (*ptr).requires_grad {
            (*ptr).data = (*ptr).data - lr * (*ptr).grad;
        }
        i = i + 1;
    }
}

// ==================== Example: Simple Neuron ====================

fn main() {
    // Create tape
    let mut tape: Tape = Tape::new();

    // Parameters: w1, w2, b
    let w1: *Value = tape.var(0.5);
    let w2: *Value = tape.var(-0.3);
    let b: *Value = tape.var(0.1);

    // Inputs
    let x1: *Value = tape.const_val(2.0);
    let x2: *Value = tape.const_val(3.0);

    // Target
    let target: *Value = tape.const_val(1.0);

    // Forward: y = sigmoid(w1*x1 + w2*x2 + b)
    let wx1: *Value = tape_mul(&tape, w1, x1);
    let wx2: *Value = tape_mul(&tape, w2, x2);
    let sum1: *Value = tape_add(&tape, wx1, wx2);
    let sum2: *Value = tape_add(&tape, sum1, b);
    let pred: *Value = tape_sigmoid(&tape, sum2);

    // Loss: MSE
    let loss: *Value = mse_loss(&tape, pred, target);

    // Backward
    tape.backward(loss);

    // Update (SGD)
    // sgd_step(w1, 3, 0.1);  // Would update w1, w2, b

    // Cleanup
    tape.drop();
}
